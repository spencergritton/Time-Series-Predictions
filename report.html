<!DOCTYPE html>
<html>
<title>CS:5980:002 Term Project</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata">

<style>
    body, html {
        height: 100%;
        font-family: "Inconsolata", sans-serif;
        margin:0;
        padding:0
    }
    div.sticky {
        position: -webkit-sticky; /* Safari */
        position: sticky;
        top: 0;
    }
    html {
        scroll-behavior: smooth;
    }
</style>

<body>
    <!-- Links (sit on top) -->
    <header class="w3-display-container w3-grayscale-min">
        <div class="w3-row w3-padding w3-black w3-display-container">
            <div class="w3-quarter w3-container">
                <a href="index.html" class="w3-button w3-block w3-black">PROPOSAL</a>
            </div>
            <div class="w3-quarter w3-container">
                <a href="progress-report.html" class="w3-button w3-block w3-black">PROGRESS REPORT</a>
            </div>
            <div class="w3-quarter w3-container">
                <a href="report.html" class="w3-button w3-block w3-black">REPORT</a>
            </div>
            <div class="w3-quarter w3-container">
                <a href="contact.html" class="w3-button w3-block w3-black">CONTACT</a>
            </div>
        </div>
    </header>

    <!-- Add a background color and large text to the whole page -->
    <div class="w3-sand w3-grayscale w3-large w3-display-container" style="min-height: 100%;">

        <div class="w3-container w3-center">
            <h1>CS:5980:002 Deep Learning</h1>
            <h1>Report</h1>
            <h5>Evaluating the Efficacy of Modern Recurrent Neural Net Architectures on Time Series Regressions</h5>
            <h6>Spencer Gritton</h6>
            <h6><a href="https://github.com/spencergritton/Time-Series-Predictions">GitHub Project</a></h6>
            <hr>
        </div>

        <div class="w3-container">
            <h1>Introduction</h1>
            <p>
                This report is an examination and comparison of popular recurrent neural network (RNN) 
                cell types and their efficacy in regression analysis on time series financial data. 
                The purpose of this report is to give normal machine learning practitioners a better idea of 
                what RNN cells are best used for which tasks with the hope that this will reduce total training 
                times for these individuals. This report consists 
                of three experiments designed to test the following RNN cell types in a variety of financial regression tasks: 
                Simple RNN, LSTM, GRU, Bi-Directional LSTM, and Bi-Directional GRU.
            </p>
        </div>
        <hr>

        <div class="w3-container">
            <h1 id="related-work">Related Work</h1>
            <p>
                In order to begin working on this research, it was necessary to research the current work in financial time series regressions. 
            </p>
            <p>
                The first report I read on this topic was "<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.742.3491&rep=rep1&type=pdf">A Neural Network Approach to Financial Forecasting</a>" written by: 
                P. Enyindah and Onwuachu Uzochukwu C.
                In this paper the authors are attempting to predict loan rates based on financial data using LSTM networks. Although Enyindah and Onwuachu 
                aren't making the same types of predictions as this paper, their work helped me to further understand this area and gain an appreciation of what may 
                be possible.
            </p>
            <p>
                The next report I read was focused specifically on financial time series predictions on the S&P 500. This report was "<a href="http://www.diva-portal.org/smash/get/diva2:1213449/FULLTEXT01.pdf">The accuracy of the LSTM model for predicting the S&P 500 index 
                    and the difference between prediction and backtesting</a>" written by: War Ahmed and Mehrdad Bahador.
                This report focused on regression analysis on the S&P 500. These authors used an input sequence of the last 22 days to attempt to predict 
                the next days closing price using 68 years worth of training data. In this paper the authors were able to train a netwok to complete 
                this task with a mean squared error of 0.00056 on the testing set. Although the report you are reading is focused on predicting 
                output sequences on a minute by minute basis instead of individual values on a day by day basis, the paper written by Ahmed and Bahador 
                highlighted the possibilites of using neural networks to make time series regressions and what sort of results 
                to strive for.
            </p>
            <br/>

            <h5>Comparison Between this Report and Related Works</h5>
            <p>
                In the majority of work in this area the papers are focused solely on achieving state of the art results 
                in their specific regression task. This research is instead focused on helping everyday practitioners 
                to choose the best RNN cell type for the task. This goal is accomplished by testing each RNN cell type in 
                a variety of ways to see how they best apply to a range of tasks.
            </p>
        </div>
        <hr>

        <!---->

        <div class="w3-container">
            <h1 id="experiment-1">Convergence Rate on Two Variable Regression (Experiment 1)</h1>
            <p>
                This experiment was designed to explore if there is a clear choice in RNN network architecture on time series regressions in terms of convergence rate. 
                The proposed benefit of this experiment is to give future machine learning practitioners a smaller subset of viable 
                RNN architectures to work with that will converge quickly while still producing viable results.
            </p>
        </div>

        <div class="w3-container">
            <h3>Dataset Details</h3>
            <p>
                This dataset is taken from a subset of the dataset "Bitcoin Historical Data", retrieved from:
                <a href="https://www.kaggle.com/mczielinski/bitcoin-historical-data">Kaggle.com</a>.
                The subset used for this experiment contains the closing price and volume of bitcoin traded each minute from:
                2018-06-14 04:31:00 - 2018-08-25 11:40:00 on the Coinbase coin exchange.
            </p>
            <br>

            <h5 id='sequence-e1'>Sequence Extraction</h5>
            <p>
                To create the training and validation sets of data the dataset was split into: 
                training input, training output, validation input, and validation output.
                The input of both sets was created as a list of 30 unit sequences.
                Each sequence in the input consisted of 30 objects from x₁ to x₃₀.
                Each xᵢ from x₁ to x₃₀ consisted of a tuple where the first element was the bitcoin price
                at x₃₀-xᵢ and the second element was the bitcoin volume at x₃₀-xᵢ. The output of both sets was a
                list of floating point values where each element in the list was the bitcoin price at x₄₀ 
                (ten minute after the sequence end) for each respective
                sequence ranging from x₁ to x₃₀. All data points in all sets had a z-score normalization applied to them
                before being placed in their respective sequences. The input and output sets were then split into training and 
                validation sets, where the validation set was simply the the most recent 10% of the time series data and the
                training set was all data that came before the 10% cut-off. An example of the sequence generation technique
                is given below for clarity. Note: This example is changed slightly from the code for clarity.
            </p>
            <p><u>Input data</u></p>
            <table class="w3-table w3-striped w3-bordered w3-border">
                <tr>
                    <th>Date</th>
                    <th>Closing Price</th>
                    <th>Volume</th>
                </tr>
                <tr>
                    <td>2018-08-25 11:50:00</td>
                    <td>5640.55</td>
                    <td>103.97</td>
                </tr>
                <tr>
                    <td>...</td>
                    <td>...</td>
                    <td>...</td>
                </tr>
                <tr>
                    <td>2018-08-25 11:40:00</td>
                    <td>5640.12</td>
                    <td>104.44</td>
                </tr>
                <tr>
                    <td>...</td>
                    <td>...</td>
                    <td>...</td>
                </tr>
                <tr>
                    <td>2018-08-25 11:11:00</td>
                    <td>5127.26</td>
                    <td>168.52</td>
                </tr>
                <tr>
                    <td>2018-08-25 11:10:00</td>
                    <td>5123.66</td>
                    <td>126.45</td>
                </tr>
                <tr>
                    <td>...</td>
                    <td>...</td>
                    <td>...</td>
                </tr>
            </table>
            
            <p><u>Generated Input and Output</u>: Note: This is just an example using the data above.</p>
            <p>Un-normalized input: [(5123.66, 126.45), (5127.26, 168.52), ..., (5640.12, 104.44)]</p>
            <p>Un-normalized output: [5640.55]</p>
            <p>Normalized input: [(0.4567, 0.556), (0.46776, 0.7823), ..., (0.8891, 0.3643)]</p>
            <p>Normalized output: [0.8912]</p>
            <p>
                Note: The above is an example of the generation of just one sequence. This dataset contained 97,774 rows of data.
                This means that roughly 97,744 sequences were generated for the inputs and outputs of this set.
            </p>
        </div>

        <div class="w3-container">
            <h3>Procedure</h3>
            <p>
                The procedure for this experiment was to test the following RNN architecture types against each other:
                Simple RNNs, LSTMs, GRUs, Bi-Directional LSTMs, and Bi-Directional GRUs. This was done by training four networks
                of each architecture type where the only difference in each network was the type of RNN cell used and number of layers.
                The goal of each network is to take a sequence of 30 minutes of normalized bitcoin price and volume data 
                then predict what the price will be 10 minutes into the future.
            </p>
        </div>

        <div class="w3-container">
            <h3>Methods</h3>
            <p>
                For each RNN type (Simple RNN, LSTM, GRU, Bi-Directional LSTM, and Bi-Directional GRU), four networks were created and trained, 
                combining to 20 trained networks.
                Each network was trained for five epochs with mean squared error as the loss function.
                The following are the four types of networks generated for each type of RNN:
                <ul>
                    <li>
                        2 RNN Layers with 64 Cells. Architecture below:
                        <ul>
                            <li>RNN with 64 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>RNN with 64 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>32 neuron fully connected layer, ReLu activation, 20% dropout, batch normalization</li>
                            <li>1 neuron fully connected layer, Linear activation.</li>
                        </ul>
                    </li>
                    <li>
                        2 RNN Layers with 128 Cells
                        <ul>
                            <li>RNN with 128 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>RNN with 128 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>32 neuron fully connected layer, ReLu activation, 20% dropout, batch normalization</li>
                            <li>1 neuron fully connected layer, Linear activation.</li>
                        </ul>
                    </li>
                    <li>
                        3 RNN Layers with 64 Cells
                        <ul>
                            <li>RNN with 64 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>RNN with 64 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>RNN with 64 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>32 neuron fully connected layer, ReLu activation, 20% dropout, batch normalization</li>
                            <li>1 neuron fully connected layer, Linear activation.</li>
                        </ul>
                    </li>
                    <li>
                        3 RNN Layers with 128 Cells
                        <ul>
                            <li>RNN with 128 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>RNN with 128 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>RNN with 128 cells, Tanh activation, 20% dropout, batch normalization</li>
                            <li>32 neuron fully connected layer, ReLu activation, 20% dropout, batch normalization</li>
                            <li>1 neuron fully connected layer, Linear activation.</li>
                        </ul>
                    </li>
                </ul>
            </p>
            <br/>

            <p style="white-space: pre-line;">Detailed layer by layer information for each network listed here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/architectures/SimpleRNN.md"><u>Simple RNN</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/architectures/LSTM.md"><u>LSTM</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/architectures/GRU.md"><u>GRU</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/architectures/Bi-DirectionalLSTM.md"><u>Bi-Directional LSTM</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/architectures/Bi-DirectionalGRU.md"><u>Bi-Directional GRU</u></a>
            </p>
            <p>

            </p>
            <p style="white-space: pre-line;">Code for dataset generation and model training here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/SimpleRNN.py"><u>Simple RNN Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/LSTM.py"><u>LSTM Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/GRU.py"><u>GRU Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/BidirectionalLSTM.py"><u>Bi-Directional LSTM Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-1/BidirectionalGRU.py"><u>Bi-Directional GRU Code</u></a>
            </p>
        </div>

        <div class="w3-container">
            <h3>Results</h3>
            <p>
                The results of training all the networks for 5 epochs with a batch size of 32 on the given task are below.
            </p>

            <table class="w3-table w3-striped w3-bordered w3-border">
                <tr>
                    <th>Network</th>
                    <th>Mean Squared Error (Validation)</th>
                    <th>Mean Absolute Error (Validation)</th>
                    <th>Training Time</th>
                </tr>
                <tr><td>Simple RNN, 2 Layer, 64 Cell</td><td>0.02484</td><td>0.0959</td><td>7 Minutes 22 Seconds</td></tr>
                <tr><td>Simple RNN, 2 Layer, 128 Cell</td><td>0.02497</td><td>0.1249</td><td>10 Minutes 57 Seconds</td></tr>
                <tr><td>Simple RNN, 3 Layer, 64 Cell</td><td>0.0338</td><td>0.1349</td><td>15 Minutes 50 Seconds</td></tr>
                <tr><td>Simple RNN, 3 Layer, 128 Cell</td><td>0.03482</td><td>0.1182</td><td>21 Minutes 8 Seconds</td></tr>
                <tr><td>LSTM, 2 Layer, 64 Cell</td><td>0.02912</td><td>0.103</td><td>15 Minutes 20 Seconds</td></tr>
                <tr><td>LSTM, 2 Layer, 128 Cell</td><td>0.02567</td><td>0.09062</td><td>30 Minutes 0 Seconds</td></tr>
                <tr><td>LSTM, 3 Layer, 64 Cell</td><td>0.03633</td><td>0.1082</td><td>28 Minutes 54 Seconds</td></tr>
                <tr><td>LSTM, 3 Layer, 128 Cell</td><td>0.03719</td><td>0.109</td><td>39 Minutes 30 Seconds</td></tr>
                <tr><td>GRU, 2 Layer, 64 Cell</td><td>0.02639</td><td>0.09239</td><td>19 Minutes 9 Seconds</td></tr>
                <tr><td>GRU, 2 Layer, 128 Cell</td><td>0.03013</td><td>0.096</td><td>30 Minutes 11 Seconds</td></tr>
                <tr><td>GRU, 3 Layer, 64 Cell</td><td>0.04415</td><td>0.144</td><td>29 Minutes 33 Seconds</td></tr>
                <tr><td>GRU, 3 Layer, 128 Cell</td><td>0.0297</td><td>0.09875</td><td>36 Minutes 41 Seconds</td></tr>
                <tr><td>Bi-Directional LSTM, 2 Layer, 64 Cell</td><td>0.03105</td><td>0.09998</td><td>29 Minutes 52 Seconds</td></tr>
                <tr><td>Bi-Directional LSTM, 2 Layer, 128 Cell</td><td>0.03778</td><td>0.1285</td><td>41 Minutes 19 Seconds</td></tr>
                <tr><td>Bi-Directional LSTM, 3 Layer, 64 Cell</td><td>0.03018</td><td>0.09699</td><td>44 Minutes 13 Seconds</td></tr>
                <tr><td>Bi-Directional LSTM, 3 Layer, 128 Cell</td><td>0.03405</td><td>0.1081</td><td>25 Minutes 50 Seconds</td></tr>
                <tr><td>Bi-Directional GRU, 2 Layer, 64 Cell</td><td>0.0371</td><td>0.1199</td><td>10 Minutes 46 Seconds</td></tr>
                <tr><td>Bi-Directional GRU, 2 Layer, 128 Cell</td><td>0.0322</td><td>0.1061</td><td>16 Minutes 21 Seconds</td></tr>
                <tr><td>Bi-Directional GRU, 3 Layer, 64 Cell</td><td>0.0386</td><td>0.1285</td><td>16 Minutes 27 Seconds</td></tr>
                <tr><td>Bi-Directional GRU, 3 Layer, 128 Cell</td><td>0.0330</td><td>0.1120</td><td>25 Minutes 4 Seconds</td></tr>
                <tr><th>Total Training Time (Minutes)</th><td></td><td></td><th>485 Minutes 567 Seconds</th></tr>
                <tr><th>Total Training Time (Hours)</th><td></td><td></td><th>8.24 Hours</th></tr>
                <tr><th>Average Training Time (Minutes)</th><td></td><td></td><th>24 Minutes 43 Seconds</th></tr>
            </table>
        </div>
        <br>

        <div class="w3-container">
            <h3>Discussion</h3>

            <h5>The more advanced architectures do not converge the fastest</h5>
            <p>
                In this experiment the simple 2 layer RNN with 64 cells per layer had the lowest mean squared error on the validation set.
                This was very surprising as I had expected the newer/more advanced networks to very easily surpass the default RNN. 
                This experiment shows it is pertinent to experiment with many different network types before settling on one.
                It might be the case though that the more advanced RNN cells are able to better generalize in the long run and 
                simply converge slower than the more simple networks.
            </p>

            <h5>Training times don't increase much with complexity</h5>
            <p>
                Running for only five epochs per experiment took nearly 9 hours of total compute time on an enterprise grade NVIDIA Quadro P4000 GPU.
                These training sessions take a long time, but with the exception of the Simple RNN, 
                the majority of RNN types take a very similar amount of time. This means that for the most
                part when training an RNN on a relatively small amount of time series data there should be little need to optimize for architectures with fast 
                training times.
                <table class="w3-table w3-striped w3-bordered w3-border">
                    <tr>
                        <th>RNN Type</th>
                        <th>Average Training Time (5 Epochs)</th>
                    </tr>
                    <tr><td>Simple RNN</td><td>13.82 Minutes</td></tr>
                    <tr><td>LSTM</td><td>28.43 Minutes</td></tr>
                    <tr><td>GRU</td><td>28.89 Minutes</td></tr>
                    <tr><td>Bi-Directional LSTM</td><td>35.31 Minutes</td></tr>
                    <tr><td>Bi-Directional GRU</td><td>27.97 Minutes</td></tr>
                </table>
            </p>

            <h5>This problem may not have been well suited to all architecture types</h5>
            <p>
                The more modern RNN architectures make use of different memory techniques to increase accuracy. Because of this, it is expected that in tasks
                where the network needs to know relevant information from many different parts of the sequence, a Simple RNN would do much worse than the 
                other types of RNN cells experimented on here. It might be just coincidence that the Simple RNNs outperformed all other architectures in this experiment
                but it also may be that this test was most easily solvable by a network with no memory and a simple output heurestic. 
                This experiment has provided some evidence that the simple RNN cell type is the best choice for quick training times 
                and acceptable results in a somewhat simple use case.
            </p>

        </div>
        <hr>

        <!---->

        <div class="w3-container">
            <h1 id="experiment-2">Model Generalization on a Five Variable Sequence Regression (Experiment 2)</h1>
            <p>
                This experiment was designed to test how otherwise equivalent models using different RNN cell types generalize on a time series
                dataset when training until a validation loss plateau.
            </p>
        </div>

        <div class="w3-container">
            <h3>Dataset Details</h3>
            <p>
                This dataset is the same subset of the "Bitcoin Historical Data", from
                <a href="https://www.kaggle.com/mczielinski/bitcoin-historical-data">Kaggle.com</a>
                used in the first experiment but altered as explained below.
            </p>
            <br/>

            <h5>Dataset generation</h5>
            <p>
                This dataset is generated in the same way as it was in <a href='#experiment-1'>Experiment 1</a>
                with several small alterations:
                <ul>
                    <li>The dataset now includes: 
                        opening price, closing price, high price, low price, and volume.</li>
                    <li>Input sequences are now 180 minutes instead of 30.</li>
                    <li>The output is now a sequence of the next 30 minutes instead of a single data point 10 minutes into the future.</li>
                    <li>The dataset is now generated using a script and stored in a .pickle file due to the dataset size. Find the script 
                        <a href='https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/data-generator.py'>here</a>.</li>
                </ul>
            </p>
        </div>

        <div class="w3-container">
            <h3>Procedure</h3>
            <p>
                The procedure for this experiment was to test the generalization ability of the following RNN cell types: 
                Simple RNNs, LSTMs, GRUs, Bi-Directional LSTMs, and Bi-Directional GRUs. This was done by training five identical
                networks for 100 epochs where the only difference between each was the type of RNN cell used. The goal of each network is to predict the 
                closing price for the next 30 minutes if given the opening price, closing price, high price, low price, and volume for each
                of the last 180 minutes.
            </p>
        </div>

        <div class="w3-container">
            <h3>Methods</h3>
            <p>
                All five networks used the following architecture with different RNN cell types:
                <ul>
                    <li>1 RNN Cell Layer (of the specific type) with 180 Cells. Tanh activation. 50% Dropout. Batch normalization. L2 regularization at 0.01.</li>
                    <li>1 Dense layer with 180 neurons. ReLu activation. 50% Dropout. Batch normalization. L2 regularization at 0.01.</li>
                    <li>1 Dense output layer with 30 neurons. Linear activation.</li>
                </ul>

                Each network used the following parameters and callbacks:
                <ul>
                    <li>Epochs: 100</li>
                    <li>Batch Size: 64</li>
                    <li>Optimizer: Adadelta: learning_rate=1.0, rho=0.95</li>
                    <li>Loss function: Mean squared error</li>
                    <li>Decrease learning rate by 40% for every 2 epochs that the validation loss doesn't decrease</li>
                    <li>Stop early if the network doesn't improve validation loss for 6 epochs (this was necessary due to the cost of training these networks)</li>
                    <li>Keras, Tensorflow, and Python random parameters were set to the same seeds to test these networks using the exact 
                        same training process
                    </li>
                </ul>
            </p>
            <p style="white-space: pre-line;">Detailed layer by layer information for each network listed here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/architectures/SimpleRNN.md"><u>Simple RNN</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/architectures/LSTM.md"><u>LSTM</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/architectures/GRU.md"><u>GRU</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/architectures/Bi-DirectionalLSTM.md"><u>Bi-Directional LSTM</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/architectures/Bi-DirectionalGRU.md"><u>Bi-Directional GRU</u></a>
            </p>
            <p>

            </p>
            <p style="white-space: pre-line;">Code for dataset generation located here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/data-generator.py"><u>Data-Generator</u></a>

                Code for model creation and training located here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/SimpleRNN.py"><u>Simple RNN Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/LSTM.py"><u>LSTM Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/GRU.py"><u>GRU Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/BiDirectionalLSTM.py"><u>Bi-Directional LSTM Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/BiDirectionalGRU.py"><u>Bi-Directional GRU Code</u></a>
            </p>
        </div>

        <div class="w3-container">
            <h3>Results</h3>
            <p>
                The results of training all five different networks with the above given parameters are given below.
            </p>

            <table class="w3-table w3-striped w3-bordered w3-border">
                <thead>
                    <tr>
                        <th>Name</th>
                        <th>Validation Loss (MSE)</th>
                        <th>Validation Mae</th>
                        <th>Epochs Scheduled</th>
                        <th>Epochs Ran</th>
                        <th>Training Time (Minutes)</th>
                    </tr>
            </thead>
                <tbody>
                    <tr>
                        <tr>
                            <td>SimpleRNN-180-30</td>
                            <td >0.003379579</td>
                            <td >0.028238779</td>
                            <td >250</td>
                            <td >117</td>
                            <td >17.21951702</td>
                        </tr>
                        <td>LSTM-180-30</td>
                        <td >0.003990279</td>
                        <td >0.029511815</td>
                        <td >250</td>
                        <td >44</td>
                        <td >15.61718321</td>
                    </tr>
                    <tr>
                        <td>GRU-180-30</td>
                        <td >0.003008778</td>
                        <td >0.025924904</td>
                        <td >250</td>
                        <td >128</td>
                        <td >32.34058838</td>
                    </tr>
                    <tr>
                        <td>BiDirectionalLSTM-180-30</td>
                        <td >0.005633137</td>
                        <td >0.038524002</td>
                        <td >250</td>
                        <td >35</td>
                        <td >13.83588647</td>
                    </tr>
                    <tr>
                        <td>BiDirectionalGRU-180-30</td>
                        <td >0.010775859</td>
                        <td >0.03390241</td>
                        <td >250</td>
                        <td >44</td>
                        <td >10.26025866</td>
                    </tr>
                    <tr>
                        <th>Total Training Time</th>
                        <td ></td><td ></td><td ></td><td ></td><td >89.27343374</td>
                    </tr>
                </tbody>
            </table>
            <br/>

            <div class="w3-center">
                <h3>Training Process Visualized</h3>
            </div>

            <div class="w3-half w3-center">
                <h5>Simple RNN</h5>
                <img style="width: 70%;" 
                    src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/plots/SimpleRNN-180-30.png?raw=true" 
                    alt="Simple RNN">
            </div>
            <div class="w3-half w3-center">
                <h5>LSTM</h5>
                <img style="width: 70%;" 
                    src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/plots/LSTM-180-30.png?raw=true" 
                    alt="LSTM">
            </div>
            <div class="w3-half w3-center">
                <h5>GRU</h5>
                <img style="width: 70%;" 
                    src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/plots/GRU-180-30.png?raw=true" 
                    alt="GRU">
            </div>
            <div class="w3-half w3-center">
                <h5>Bi-Directional LSTM</h5>
                <img style="width: 70%;" 
                    src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/plots/BiDirectionalLSTM-180-30.png?raw=true" 
                    alt="Bi-Directional LSTM">
            </div>
            <div class="w3-center">
                <h5>Bi-Directional GRU</h5>
                <img style="width: 40%;" 
                    src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-2/plots/BiDirectionalGRU-180-30.png?raw=true" 
                    alt="Bi-Directional GRU">
            </div>
        </div>

        <div class="w3-container">
            <h3>Discussion</h3>
            <h5>All Networks Reacted Similarly to the Data</h5>
            <p>
                Because the random seeds in Keras and Tensorflow were set to the same for each experiment the training process was the 
                exact same for all five networks. If you look closely at the graphs above you will be able to see this. When validation loss 
                increases in one graph it tends to increase in the others at different scales. 
            </p>
            <br>

            <h5>Bi-Directional Networks Performed the Worst</h5>
            <p>
                Similar to experiment 1, the bi-directional networks performed the worst in terms of validation loss. This is interesting because 
                the common expectation would be that with access to more information the networks would improve and yet that is not the case. 
                The reason for this is unclear but leads to several possibilites:
                <ul>
                    <li>Bi-Directional networks are not as well suited as others for financial time series regressions.</li>
                    <li>These specific network architectures are not very deep. It's possible bi-directional RNN cells would perform better 
                        in a deeper network.
                    </li>
                    <li>The networks were stopped early if they stopped showing progress due to cost limitations. If these limitations were 
                        not in place it may be the case that the bi-directional networks would have continued to learn and out perform the other 
                        networks.
                    </li>
                </ul>
            </p>
            <br/>

            <h5>LSTM, GRU, and Simple RNN Networks Performed the Best</h5>
            <p>
                These networks did the best in the experiment but there are several key differences between them:
                <ol>
                    <li>The LSTM network trained for nearly 80 less epochs than the other two networks.</li>
                    <li>The GRU network trained for twice as long as the other networks.</li>
                    <li>The Simple RNN network did just as well as the other two networks, much like in experiment 1.</li>
                </ol>
            </p>
            <br/>

            <h5>The Simple RNN Network Performed Very Well</h5>
            <p>
                The most interesting thing to take away from this experiment is that the simple RNN network did just as well as the others 
                and trained at a much faster epoch / minute speed than any other network type. Using the results from experiment 1 and 2 combined 
                it seems that using a simple RNN may be the best option for a neural network with a low amount of layers doing a time series 
                regression.
            </p>
            <p>
                In <a href="#experiment-3">experiment 3</a> deeper RNN networks are tested against each other using a dataset 15 times as large as 
                what was used for this experiment. This is to test if simple RNNs are superior for all financial time series regressions or if they 
                simply perform well with a low amount of data and somewhat shallow network design.
            </p>
        </div>
        <hr>

        <!---->

        <div class="w3-container">
            <h1 id="experiment-3">Deep Model Generalization on a Seven Variable Sequence Regression (Experiment 3)</h1>
            <p>
                This experiment was designed to test how different RNN cell types would generalize when using a very large dataset 
                and deeper network architectures. This experiment was specifically added to the project to test if simple RNNs were clearly 
                superior to other RNN cell types in terms of their training time to validation accuracy ratio on this sort of problem set 
                or if the other cell types simply needed more data and/or training time.
            </p>
        </div>

        <div class="w3-container">
            <h3>Dataset Details</h3>
            <p>
                This dataset is the entire coinbase section of "Bitcoin Historical Data", from
                <a href="https://www.kaggle.com/mczielinski/bitcoin-historical-data">Kaggle.com</a>.
                Unlike the previous two experiments, this experiment uses all 1,000,000 rows of the dataset as input and considers 
                all seven given data features.
            </p>
            <p>
                Note:
                <ul>
                    <li>The input sequences to this network are the last 180 minutes of bitcoin data where each minute has seven features: 
                        opening price, closing price, high price, low price, volume in bitcoin, volume in dollars, and weighted price.
                    </li>
                    <li>The output sequences of this network are the next 30 minutes of closing price data (these are normalized using 
                        the datasets normalization for closing price).
                    </li>
                </ul>
            </p>
            <br/>

            <h5>Dataset generation</h5>
            <p>
                This dataset is generated in the same way as it was in <a href='#experiment-2'>Experiment 2</a>
                with several small alterations:
                <ul>
                    <li>The dataset now includes: 
                        volume in Bitcoin (this was the volume category in experiment 1 and 2), volume in dollars, and weighted price.</li>
                    <li>The dataset is now generated using an improved script for efficiency. It is located 
                        <a href='https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/data-generator.py'>here</a>.</li>
                </ul>
            </p>
        </div>

        <div class="w3-container">
            <h3>Procedure</h3>
            <p>
                The procedure for this experiment was to test how different RNN cell types would generalize to a financial time series regression 
                when given a large dataset, ample time to train, and a deeper network architecture. This was done by training 3 identitical networks 
                for 250 epochs on a large multi-year dataset where the only difference in the networks are their RNN cell types. The goal of each of 
                these networks is to take the last 180 minutes of bitcoin data (in the seven features listed above) and output a sequence of the next 
                30 minutes of bitcoin closing prices.
            </p>
            <p>
                The three networks used for this experiment were:
                <ul>
                    <li>A deep network of stacked simple RNN cells</li>
                    <li>A deep network of stacked LSTM and GRU cells</li>
                    <li>A deep network of stacked Bi-Directional LSTM and GRU cells</li>
                </ul>
            </p>
        </div>

        <div class="w3-container">
            <h3>Methods</h3>
            <p>
                All three networks used the following architecture with different RNN cell types:
                <ul>
                    <li>1 RNN Cell Layer (of the specific type) with 180 Cells. Tanh activation. 50% Dropout. Batch normalization. L2 regularization at 0.01.</li>
                    <li>1 RNN Cell Layer (of the specific type) with 180 Cells. Tanh activation. 50% Dropout. Batch normalization. L2 regularization at 0.01.</li>
                    <li>1 Dense layer with 1,800 neurons. ReLu activation. 50% Dropout. Batch normalization. L2 regularization at 0.01.</li>
                    <li>1 Dense layer with 180 neurons. ReLu activation. 50% Dropout. Batch normalization. L2 regularization at 0.01.</li>
                    <li>1 Dense output layer with 30 neurons. Linear activation.</li>
                </ul>

                Each network used the following parameters and callbacks:
                <ul>
                    <li>Epochs: 250</li>
                    <li>Batch Size: 256</li>
                    <li>Optimizer: Adadelta: learning_rate=1.0, rho=0.95</li>
                    <li>Loss function: Mean squared error</li>
                    <li>Decrease learning rate by 40% for every 6 epochs that the validation loss doesn't decrease</li>
                    <li>Stop early if the network doesn't improve validation loss for 30 epochs (this was necessary due to the cost of training these networks)</li>
                    <li>Keras, Tensorflow, and Python random parameters were set to the same seeds to test these networks using the exact 
                        same training process
                    </li>
                </ul>
            </p>
            <p style="white-space: pre-line;">Detailed layer by layer information for each network listed here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/architectures/SimpleRNN.md"><u>Simple RNN</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/architectures/LSTM-GRU.md"><u>LSTM-GRU</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/architectures/Bi-Directional-LSTM-GRU.md"><u>Bi-Directional LSTM-GRU</u></a>
            </p>
            <p>

            </p>
            <p style="white-space: pre-line;">Code for dataset generation located here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/data-generator.py"><u>Data-Generator</u></a>

                Code for model creation and training located here:
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/SimpleRNN.py"><u>Simple RNN Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/LSTM-GRU.py"><u>LSTM-GRU Code</u></a>
                <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/BiDirectional-LSTM-GRU.py"><u>Bi-Directional LSTM-GRU Code</u></a>
            </p>
        </div>

        <div class="w3-container">
            <h3>Results</h3>
            <p>
                The results of training all three different networks with the above parameters are given below.
            </p>
            <table class="w3-table w3-striped w3-bordered w3-border">
                <thead><tr><th title="Field #1">Name</th>
                <th title="Field #2">Validation Loss (MSE)</th>
                <th title="Field #3">Validation Mae</th>
                <th title="Field #4">Epochs Scheduled</th>
                <th title="Field #5">Epochs Ran</th>
                <th title="Field #6">Training Time (Minutes)</th>
                </tr></thead>
                <tbody>
                    <tr>
                        <td>SimpleRNN</td>
                        <td>1.056928001</td>
                        <td>0.642972946</td>
                        <td>250</td>
                        <td>36</td>
                        <td>114.1520359</td>
                    </tr>
                    <tr>
                        <td>LSTM-GRU</td>
                        <td>0.15191308</td>
                        <td>0.23991394</td>
                        <td>250</td>
                        <td>34</td>
                        <td>177.5106781</td>
                    </tr>
                    <tr>
                        <td>Bidirectional-LSTM-GRU</td>
                        <td>0.067635128</td>
                        <td>0.129869357</td>
                        <td>250</td>
                        <td>63</td>
                        <td>450.9644763</td>
                    </tr>
                    <tr>
                        <th>Total Training Time</th>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td></td>
                        <td>742.62</td>
                    </tr>
                </tbody></table>

            <div class="w3-center">
                <h3>Training Process Visualized</h3>
            </div>

            <div class="w3-half w3-center">
                <h5>Simple RNN</h5>
                <img style="width: 70%;" 
                    src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/plots/SimpleRNN-250.png?raw=true" 
                    alt="Simple RNN">
            </div>
            <div class="w3-half w3-center">
                <h5>LSTM-GRU</h5>
                <img style="width: 70%;" 
                    src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/plots/LSTM-GRU-250.png?raw=true" 
                    alt="LSTM-GRU">
            </div>
        </div>
        <div class="w3-center">
            <h5>Bi-Directional LSTM-GRU</h5>
            <img style="width: 40%;" 
                src="https://github.com/spencergritton/Time-Series-Predictions/blob/master/experiment-3/plots/Bidirectional-LSTM-GRU-250.png?raw=true" 
                alt="BiDirectional-LSTM-GRU">
        </div>

        <div class="w3-container">
            <h3>Discussion</h3>
            <h5>The Simple RNN Network Performed Poorly</h5>
            <p>
                Although the simple RNN cell networks performed very well in experiments 1 and 2 it performed exceptionally poorly in this experiment.
                In fact, it performed about 15X worse than the bi-directional LSTM-GRU network and about 7X worse than the LSTM-GRU network when compared 
                based on mean squared error. This is contradictory to the findings of <a href="#experiment-2">Experiment 2</a> where the simple RNN network 
                performed just as well or better than the other network types.
            </p>
            <br/>

            <h5>The Bi-Directional Network Surpassed Expectations</h5>
            <p>
                In all previous experiments the bi-directional networks had some of the worst results, but in this experiment the network has the best results 
                by a large margin. Experiment 1 was focused on convergence speed and experiment 2 was focused on using a shallow network to generalize on 
                a small dataset. Past experiments showed evidence that bi-directional networks converge more slowly than other RNN cell type networks and 
                that they also don't generalize well on small datasets. This experiment shows evidence that although bi-directional networks converge slowly 
                they are able to learn more complex features from large datasets and generalize that learning better than other network types.
            </p>
            <br/>

            <h5>Bi-Directional Networks Train Slowly</h5>
            <p>
                This was shown in <a href="#experiment-1">Experiment 1</a> but is much more obvious in this experiment. The Bi-Directional network had an 
                average training time per epoch of 7.14 minutes compared to 5.20 minutes and 3.17 minutes for the LSTM-GRU and simple RNN networks. There is a clear 
                trade off between training time and accuracy in this experiment and possibly more broadly between these RNN cell types.
            </p>
        </div>
        <hr>

        <!---->

        <div class="w3-container">
            <h1>Conclusions</h1>
            <p>
                The purpose of this report was to explore the efficacy of using recurrent neural networks to make regressions on time series financial data, 
                compare each recurrent cell type based on their: training times, convergence speeds, and accuracy, and 
                see if there is an RNN cell type or types that produce superior results for this type of analysis.
            </p>
            <br>

            <h5>Convergence Rate on Two Variable Regression (Experiment 1)</h5>
            <p>
                This experiment showed that when training a relatively shallow network on a small dataset all RNN cell types are able to converge. It also led 
                to some surprising discoveries:
                <ul>
                    <li>Bi-Directional networks converge much slower (at least initially) than the networks using other RNN cell types in this task.</li>
                    <li>Networks using simple RNN cells converge quickly and have the same or better accuracy as all other RNN cell types in this task.</li>
                </ul>
            </p>
            <br>

            <h5>Model Generalization on a Five Variable Sequence Regression (Experiment 2)</h5>
            <p>
                This experiment was designed to test how each RNN cell type would perform when: given more training data,
                given more training time, and how each would converge when expected to output a sequence instead of a single value. 
                This experiment showed the following:
                <ul>
                    <li>Bi-Directional networks performed the worst at this task when compared to the other network types.</li>
                    <li>Much like experiment 1, simple RNN networks performed surprisingly well and trained very quickly.</li>
                    <li>LSTM and GRU networks performed well, but this is as expected.</li>
                </ul>
                Once again this experiment was surprising as the simple RNN network did much better than both Bi-Directional networks and was competitive with 
                both the LSMT and GRU networks. My hypothesis is that given: a relatively shallow network size, low amounts of data, and 
                the early stopping conditions of this experiment that the Bi-Directional networks either didn't have enough data or time to learn and generalize. 
                Experiment 3 was created to gain insight on this as the results did not meet my expectations.
            </p>
            <br>

            <h5>Deep Model Generalization on a Seven Variable Sequence Regression (Experiment 3)</h5>
            <p>
                This experiment was designed to test how the following network types generalized and converged when training on a large dataset and a deeper network 
                architecture: simple RNNs, LSTM/GRU combinations, and bi-directional LSTM/GRU combinations. This experiment showed drastically different results from 
                the previous two experiments:
                <ul>
                    <li>Bi-Directional networks performed far better than any other network type.</li>
                    <li>Simple RNN networks performed far worse than the other networks.</li>
                </ul>
                These results were more in line with my expectations but did not necessarily agree with the previous two experiments. My hypothesis is that 
                in order to take advantage of the benefits of Bi-Directional and/or memory cells like GRU and LSTM, the network must be sufficiently deep and/or be 
                trained with a large amount of data. This is shown by the staggering differences in results from experiment 1 and 2 to experiment 3.
            </p>
            <br>

            <h5>Experimental Results</h5>
            <p>
                When training the networks to take as input the last 180 minutes of data and output the next 30 minutes of closing price, the best networks 
                produced in this report were  
                able to achieve a mean squared error (MSE) of 0.003008778 when using a small dataset and 0.067635128 when using an extremely large dataset.
            </p>
            <p>
                In the <a href="#related-work">Related Work</a> section of the paper it was shown that authors War Ahmed and Mehrdad Bahador 
                were able to achieve a MSE of 0.00056 when predicting next day closing price of the S&P 500 from an input of the last 22 days of S&P price 
                and volume data. Although these results are not comparable at all, it is interesting to note that their MSE of 
                0.00056 * 30 days = 0.0168, which is between the MSEs of the two best networks produced as a result of this report.
            </p>
            <br>

            <h5>Next Steps</h5>
            <p>
                Given more time and ample training resources I would like to continue this research while experimenting 
                with many more neural network types, data sets, network architectures, and hyper parameter changes. 
                Increasing the scope of these experiments vastly is the only way to really give practitioners a good 
                idea of the pros and cons of each specific RNN type. I would specifically like to start experimenting with 
                encoder-decoder network types and their unique variants (seq2seq, autoencoder, etc.). Unfortunately, I don't currently possess the 
                resources (a GPU and/or the capital to rent a GPU) to proceed with these experiments.
            </p>
            <br/>

            <h5>Final Thoughts</h5>
            <p>
                These experiments were first and foremost opportunities for me to learn more about deep learning and recurrent neural networks specifically. 
                Although they were not exhaustive and may contain flaws, the experiments provided me with a great opportunity to learn at the time of their creation and 
                for that I'm thankful.
            </p>
            <p>
                Given my experimental results I offer the following advice in terms of what RNN cell should be used in what situation:
                <ul>
                    <li>If you're looking for a quickly trainable baseline, use a simple RNN cell network. It may perform far worse than other LSTM cells 
                        but it will give you an idea of what your baseline results may be and if there are any glaring issues to be solved before engaging in 
                        longer training time experiments.
                    </li>
                    <li>Bi-Directional networks seem to perform best if given a lot of data and ample training time. This is supported both by experiment 3 
                        and the paper by Ahmed and Bahador in the <a href="#related-work">Related Work</a> section of the paper.
                    </li>
                    <li>LSTMs and GRUs always seem to perform adequately (somewhere between the middle and top of the pack). These networks are a nice balance 
                        between training time and accracy as compared to Bi-Directional and simple RNN networks.
                    </li>
                    <li>One size does not fit all. As shown through my experiments there is clearly not one cell type that works best in all sitautions.
                        Try things out and see what works for your dataset and model architecture.</li>
                    <li>There are many RNN variants not explored in this report and many more deep learning models. Experiment with what works best for you.</li>
                </ul>
            </p>
            <br>

            <p>
                Thank you all for taking the time to read this research report. A lot of time was put into this project and although it is far from perfect 
                it was a great experience.
            </p>
        </div>
        <hr>

        <div class="w3-container">
            <h1>Acknowledgements</h1>
            <p><a href="https://www.youtube.com/user/sentdex">Sentdex</a> - A huge thank you goes out to this user from YouTube for their 
                comprehensive and well thought out tutorials on deep learning. Experiment 1 used a bit of code from Sentdex and I wouldn't have been able to make 
                nearly as much progress in this project without their various tutorials.
            </p>
            <p><a href="https://cmdlinetips.com/2019/10/how-to-make-a-plot-with-two-different-y-axis-in-python-with-matplotlib/">Cmdlinetips.com</a> - This website was hugely helpful in my learning to plot the results from these experiments into useful formats.
            </p>
            <p><a href="https://stackoverflow.com">Stackoverflow.com</a> - An invaluable resource for programming questions and everything else.
            </p>
            <p>
                <a href="https://keras.io/">Keras</a> and <a href="https://www.tensorflow.org/">Tensorflow</a> - Without which this project would have taken many more hours.
            </p>
            <p>
                <a href="https://www.paperspace.com/">Paperspace.com</a> - For providing cost effective cloud computing resources to train and test models.
            </p>
            <p>
                <a href="https://homepage.cs.uiowa.edu/~hzhang/">Professor Hantao Zhang</a> - For teaching this class well, even as it moved online.
            </p>
        </div>
        <hr>

        <div class="w3-container">
            <h1>Technologies Used</h1>
            <p style="white-space: pre-line;">Find more information on the specific project dependencies <a href="https://github.com/spencergritton/Time-Series-Predictions/blob/master/readme.md">here</a>.
                
                Programming Language: Python 3
                Deep Learning Framework: Tensorflow / Keras
                CSV Manipulation Library: Pandas
                Graphing Library: Matplotlib / Tensorboard
            </p>
        </div>
        <hr>

        <div class="w3-container">
            <h1>References</h1>
            <p>
                <a href="http://www.diva-portal.org/smash/get/diva2:1213449/FULLTEXT01.pdf">The accuracy of the LSTM model for predicting the S&P 500 index 
                    and the difference between prediction and backtesting </a> - War Ahmed and Mehrdad Bahador
            </p>
            <p>
                <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.742.3491&rep=rep1&type=pdf">A Neural Network Approach to Financial Forecasting</a> 
                - P. Enyindah and Onwuachu Uzochukwu C
            </p>
        </div>

    </div>
</body>

</html>